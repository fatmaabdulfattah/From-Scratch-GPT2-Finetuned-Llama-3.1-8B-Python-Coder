{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Run this cell first to set up everything\n",
        "!pip install -q pyngrok streamlit transformers peft accelerate bitsandbytes\n",
        "\n",
        "import subprocess\n",
        "import threading\n",
        "from pyngrok import ngrok, conf\n",
        "import time\n",
        "import requests\n",
        "import os\n",
        "\n",
        "# Configure ngrok with your authtoken\n",
        "# REPLACE 'YOUR_AUTHTOKEN_HERE' with your actual ngrok authtoken\n",
        "from google.colab import userdata\n",
        "token = userdata.get('NGROK_TOKEN')\n",
        "conf.get_default().auth_token = token\n",
        "\n",
        "# Function to run Streamlit in the background\n",
        "def run_streamlit():\n",
        "    # Write the Streamlit app to a file\n",
        "    app_code = '''import streamlit as st\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "\n",
        "# Set page configuration\n",
        "st.set_page_config(\n",
        "    page_title=\"Llama-3.1 Code Generator\",\n",
        "    page_icon=\"ü§ñ\",\n",
        "    layout=\"wide\"\n",
        ")\n",
        "\n",
        "# App title and description\n",
        "st.title(\"ü§ñ Llama-3.1 Code Generator\")\n",
        "st.markdown(\"Generate Python code using fine-tuned Llama-3.1 model with LoRA adapter\")\n",
        "\n",
        "# Sidebar for configuration\n",
        "with st.sidebar:\n",
        "    st.header(\"‚öôÔ∏è Generation Parameters\")\n",
        "\n",
        "    # Temperature slider\n",
        "    temperature = st.slider(\n",
        "        \"Temperature\",\n",
        "        min_value=0.1,\n",
        "        max_value=2.0,\n",
        "        value=0.7,\n",
        "        step=0.1,\n",
        "        help=\"Controls randomness: Lower = more deterministic, Higher = more creative\"\n",
        "    )\n",
        "\n",
        "    # Max length slider\n",
        "    max_length = st.slider(\n",
        "        \"Max Length\",\n",
        "        min_value=50,\n",
        "        max_value=1000,\n",
        "        value=200,\n",
        "        step=50,\n",
        "        help=\"Maximum number of tokens to generate\"\n",
        "    )\n",
        "\n",
        "    # Top-k slider\n",
        "    top_k = st.slider(\n",
        "        \"Top-k\",\n",
        "        min_value=1,\n",
        "        max_value=100,\n",
        "        value=50,\n",
        "        step=1,\n",
        "        help=\"Number of highest probability tokens to consider for sampling\"\n",
        "    )\n",
        "\n",
        "    st.markdown(\"---\")\n",
        "    st.info(\"Adjust these parameters to control the generation behavior.\")\n",
        "\n",
        "# Initialize session state for model loading\n",
        "if 'model_loaded' not in st.session_state:\n",
        "    st.session_state.model_loaded = False\n",
        "if 'model' not in st.session_state:\n",
        "    st.session_state.model = None\n",
        "if 'tokenizer' not in st.session_state:\n",
        "    st.session_state.tokenizer = None\n",
        "if 'prompt' not in st.session_state:\n",
        "    st.session_state.prompt = \"\"\n",
        "\n",
        "# Model loading section\n",
        "def load_model():\n",
        "    \"\"\"Load the model and tokenizer\"\"\"\n",
        "    try:\n",
        "        with st.spinner(\"Loading base model and tokenizer...\"):\n",
        "            base_model_name = \"unsloth/meta-llama-3.1-8b-unsloth-bnb-4bit\"\n",
        "            adapter_path = \"fatoma/Llama-3.1-8B-Python-Coder\"  # Public adapter\n",
        "\n",
        "            tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
        "            model = AutoModelForCausalLM.from_pretrained(\n",
        "                base_model_name,\n",
        "                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "                device_map=\"auto\",\n",
        "                load_in_4bit=True\n",
        "            )\n",
        "\n",
        "            with st.spinner(\"Loading LoRA adapter...\"):\n",
        "                model = PeftModel.from_pretrained(model, adapter_path)\n",
        "                model.eval()\n",
        "\n",
        "            st.session_state.tokenizer = tokenizer\n",
        "            st.session_state.model = model\n",
        "            st.session_state.model_loaded = True\n",
        "\n",
        "            st.success(\"Model loaded successfully!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error loading model: {str(e)}\")\n",
        "\n",
        "# Load model button\n",
        "if not st.session_state.model_loaded:\n",
        "    st.info(\"Click the button below to load the model (this may take a few minutes)\")\n",
        "    if st.button(\"üöÄ Load Model\"):\n",
        "        load_model()\n",
        "else:\n",
        "    st.success(\"‚úÖ Model is ready!\")\n",
        "\n",
        "# Generation function\n",
        "def generate_response(prompt, max_length, temperature, top_k):\n",
        "    \"\"\"Generate a response from the model\"\"\"\n",
        "    try:\n",
        "        inputs = st.session_state.tokenizer(prompt, return_tensors=\"pt\")\n",
        "        inputs = {k: v.to(st.session_state.model.device) for k, v in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = st.session_state.model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=max_length,\n",
        "                temperature=temperature,\n",
        "                top_k=top_k,\n",
        "                do_sample=True,\n",
        "                pad_token_id=st.session_state.tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        response = st.session_state.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        return response\n",
        "    except Exception as e:\n",
        "        return f\"Error during generation: {str(e)}\"\n",
        "\n",
        "# Main input area\n",
        "st.subheader(\"üí¨ Enter your coding prompt\")\n",
        "\n",
        "# Text input with examples - now using session state\n",
        "prompt = st.text_area(\n",
        "    \"Prompt:\",\n",
        "    height=100,\n",
        "    value=st.session_state.prompt,\n",
        "    placeholder=\"e.g., Create a function to calculate the sum of a sequence of integers...\",\n",
        "    help=\"Enter a description of the code you want to generate\"\n",
        ")\n",
        "\n",
        "# Example prompts with fixed session state handling\n",
        "col1, col2, col3 = st.columns(3)\n",
        "with col1:\n",
        "    if st.button(\"Example 1\"):\n",
        "        st.session_state.prompt = \"Write a function to reverse a string\"\n",
        "        st.rerun()\n",
        "with col2:\n",
        "    if st.button(\"Example 2\"):\n",
        "        st.session_state.prompt = \"Create a function to calculate the sum of a sequence of integers.\"\n",
        "        st.rerun()\n",
        "with col3:\n",
        "    if st.button(\"Example 3\"):\n",
        "        st.session_state.prompt = \"Create a Python program to sort and print out the elements of an array of integers\"\n",
        "        st.rerun()\n",
        "\n",
        "# Generate button\n",
        "if st.button(\"‚ú® Generate Code\", type=\"primary\", disabled=not st.session_state.model_loaded):\n",
        "    if not prompt.strip():\n",
        "        st.warning(\"Please enter a prompt first!\")\n",
        "    else:\n",
        "        with st.spinner(\"Generating code...\"):\n",
        "            response = generate_response(prompt, max_length, temperature, top_k)\n",
        "\n",
        "        # Display results\n",
        "        st.subheader(\"üìã Generated Code\")\n",
        "\n",
        "        # Create tabs for different views\n",
        "        tab1, tab2 = st.tabs([\"Formatted\", \"Raw Output\"])\n",
        "\n",
        "        with tab1:\n",
        "            # Try to extract just the code part (simplified approach)\n",
        "            code_lines = []\n",
        "            in_code_block = False\n",
        "\n",
        "            for line in response.split('\\\\n'):\n",
        "                if line.strip().startswith('```'):\n",
        "                    in_code_block = not in_code_block\n",
        "                    continue\n",
        "                if in_code_block or (line.strip() and not line.strip().startswith('#')):\n",
        "                    code_lines.append(line)\n",
        "\n",
        "            code_output = '\\\\n'.join(code_lines)\n",
        "\n",
        "            if code_output.strip():\n",
        "                st.code(code_output, language='python')\n",
        "            else:\n",
        "                st.code(response, language='text')\n",
        "\n",
        "        with tab2:\n",
        "            st.text_area(\"Raw output\", response, height=300, disabled=True)\n",
        "\n",
        "        # Display generation info\n",
        "        st.markdown(\"---\")\n",
        "        col1, col2, col3 = st.columns(3)\n",
        "        with col1:\n",
        "            st.metric(\"Temperature\", f\"{temperature:.1f}\")\n",
        "        with col2:\n",
        "            st.metric(\"Max Length\", max_length)\n",
        "        with col3:\n",
        "            st.metric(\"Top-k\", top_k)\n",
        "\n",
        "# Add some information about the model\n",
        "with st.expander(\"‚ÑπÔ∏è About this Model\"):\n",
        "    st.markdown(\"\"\"\n",
        "    **Model Details:**\n",
        "    - Base Model: `unsloth/meta-llama-3.1-8b-unsloth-bnb-4bit`\n",
        "    - Fine-tuned with LoRA adapter for Python code generation\n",
        "    - Optimized for coding tasks and Python development\n",
        "\n",
        "    **Generation Parameters:**\n",
        "    - **Temperature**: Controls randomness (0.1-2.0)\n",
        "    - **Max Length**: Maximum tokens to generate\n",
        "    - **Top-k**: Number of tokens to consider during sampling\n",
        "\n",
        "    **Note**: First generation might be slower as the model warms up.\n",
        "    \"\"\")\n",
        "\n",
        "# Footer\n",
        "st.markdown(\"---\")\n",
        "st.caption(\"Powered by Transformers, PEFT, and Streamlit\")'''\n",
        "\n",
        "    with open('app.py', 'w') as f:\n",
        "        f.write(app_code)\n",
        "\n",
        "    # Run Streamlit\n",
        "    subprocess.run(['streamlit', 'run', 'app.py', '--server.port', '8501', '--server.address', '0.0.0.0'])\n",
        "\n",
        "# Start Streamlit in a separate thread\n",
        "thread = threading.Thread(target=run_streamlit)\n",
        "thread.start()\n",
        "\n",
        "# Wait a bit for the server to start\n",
        "time.sleep(5)\n",
        "\n",
        "# Set up ngrok tunnel\n",
        "public_url = ngrok.connect(8501, bind_tls=True)\n",
        "print(\"Your Streamlit app is now available at:\")\n",
        "print(f\"üëâ {public_url}\")\n",
        "\n",
        "# Display clickable link\n",
        "from IPython.display import HTML\n",
        "display(HTML(f'<a href=\"{public_url}\" target=\"_blank\">Open Llama-3.1 Code Generator</a>'))\n",
        "\n",
        "# Keep the tunnel open\n",
        "try:\n",
        "    while True:\n",
        "        time.sleep(1)\n",
        "except KeyboardInterrupt:\n",
        "    print(\"Shutting down...\")\n",
        "    ngrok.kill()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "ti7v0a58iHa4",
        "outputId": "8cb0e16a-1664-4141-9fc4-686ed691c5bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your Streamlit app is now available at:\n",
            "üëâ NgrokTunnel: \"https://acd91d475c25.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<a href=\"NgrokTunnel: \"https://acd91d475c25.ngrok-free.app\" -> \"http://localhost:8501\"\" target=\"_blank\">Open Llama-3.1 Code Generator</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shutting down...\n"
          ]
        }
      ]
    }
  ]
}